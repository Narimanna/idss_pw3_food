{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess FOOD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "build_ingredient_list = False\n",
    "base_dir = os.getcwd()+\"/food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Variables\n",
    "ingredient_map = {\"count\": 0, \"children\": dict(), \"words\": \"\"}\n",
    "all_recipes = dict()\n",
    "all_ingredients = dict()\n",
    "unknown_ingredients = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load NLTK\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "\treturn tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "\treturn tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "\treturn tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "\treturn tag in ['JJ', 'JJR', 'JJS']\n",
    "\t\n",
    "def penn_to_wn(tag):\n",
    "\tif is_adjective(tag):\n",
    "\t\treturn wn.ADJ\n",
    "\telif is_noun(tag):\n",
    "\t\treturn wn.NOUN\n",
    "\telif is_adverb(tag):\n",
    "\t\treturn wn.ADV\n",
    "\telif is_verb(tag):\n",
    "\t\treturn wn.VERB\n",
    "\treturn None\n",
    "\n",
    "def lemmatize_sentence(sentence, includePos=False):\n",
    "\tlemmatized=[]\n",
    "\tsentence = nltk.word_tokenize(sentence.replace(\"*\",\"\"))\n",
    "\t# Tokenize sentence\n",
    "\tsentence = nltk.pos_tag(sentence)\n",
    "\tfor token in sentence:\n",
    "\t\twn=penn_to_wn(token[1]);\n",
    "\t\tif wn is not None:\n",
    "\t\t\tlemma = wordnet_lemmatizer.lemmatize(token[0], wn)\n",
    "\t\telse:\n",
    "\t\t\tlemma = wordnet_lemmatizer.lemmatize(token[0])\n",
    "\t\tif includePos:\n",
    "\t\t\tlemmatized.append((token[0], token[1], lemma.lower()))\n",
    "\t\telse:\n",
    "\t\t\tlemmatized.append(lemma.lower())\n",
    "\treturn lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Constants\n",
    "invalid_tokens = [\n",
    "\t\"(\",\")\", \"[\", \"]\", \"cup\", \"¾\", \"t\", \"ml\", \"l\", \"'\", \"c\", \"i\", \"a\", \"tablespoon\", \"ounce\", \"grain\", \";\", \"+\",\n",
    "\t\"of\", \"%\", \",\", \".\", \"tsp\", \"tbsp\", \"pound\", \"pinch\", \"g\", \"oz\", \"tbsps\", \"tsps\", \":\", \"tin\", \"the\", \"de\", \"pot\",\n",
    "\t\"tb\", \"lb\", \"package\", \"teaspoon\", \"s\", \"½\", \"ozs\", \"or\", \"and\", \"for\", \"in\", \"x\", \"tb\", \"*\", \"can\", \"pkg\", \"di\"\n",
    "]\n",
    "valid_nutrition_facts = [\n",
    "\t\"ENERC_KCAL\", \"FAT\", \"FASAT\",\"CHOCDF\",\"FIBTG\",\"PROCNT\",\n",
    "    \"CHOLE\",\"NA\",\"CA\",\"MG\",\"K\",\"FE\",\"ZN\",\"P\",\"VITA_RAE\",\n",
    "    \"VITC\",\"RIBF\",\"VITB6A\",\"VITB12\",\"VITD\",\"TOCPHA\",\"VITK1\"\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionaries\n",
    "\n",
    "def build_ingredients_dictionary():\n",
    "\twith open(base_dir+\"/../ingredient_dataset.txt\") as recipe_dataset:\n",
    "\t\tall_ingredients = dict()\n",
    "\t\tfor line in recipe_dataset:\n",
    "\t\t\tline = line.strip().lower()\n",
    "\t\t\tif line in all_ingredients:\n",
    "\t\t\t\tall_ingredients[line]=all_ingredients[line]+1\n",
    "\t\t\telse:\n",
    "\t\t\t\tall_ingredients[line]=1\n",
    "\n",
    "\t\tall_ingredients_sorted = sorted(all_ingredients.items(), key=lambda ing: -len(ing[0])*1000000+ing[1])\n",
    "\t\twith open('ingredients.txt', 'w') as ingredients_file:\n",
    "\t\t\tfor ingredient in all_ingredients_sorted:\n",
    "\t\t\t\tingredients_file.write(str(ingredient[1])+\" \"+ingredient[0]+\"\\n\")\n",
    "\t\treturn all_ingredients_sorted\n",
    "\n",
    "if build_ingredient_list:\n",
    "\tall_ingredients = build_ingredients_dictionary()\n",
    "else:\n",
    "\twith open('ingredients.txt') as ingredients:\n",
    "\t\tfor line in ingredients:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tingredient = line.split();\n",
    "\t\t\tall_ingredients[\" \".join(ingredient[1:])] = int(ingredient[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ingredient management\n",
    "def get_ingredient(line, imap):\n",
    "\tparts = lemmatize_sentence(line);\n",
    "\tingredient =  aux_get_ingredient(parts, imap)\n",
    "\tif ingredient is None:\n",
    "\t\treturn aux_get_ingredient(nltk.word_tokenize(line),imap)\n",
    "\telse:\n",
    "\t\treturn ingredient\n",
    "\n",
    "def aux_get_ingredient(parts, imap):\n",
    "\tfor part in parts:\n",
    "\t\tif part in imap[\"children\"]:\n",
    "\t\t\taux = aux_get_ingredient(parts, imap[\"children\"][part])\n",
    "\t\t\tif aux is not None:\n",
    "\t\t\t\treturn aux\n",
    "\tif imap[\"count\"] > 0:\n",
    "\t\treturn imap\n",
    "\telse:\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the ingredient map\n",
    "index=0\n",
    "for line in all_ingredients:\n",
    "\tparts = line.split()\n",
    "\taux = ingredient_map\n",
    "\tfor part in parts:\n",
    "\t\tif part in aux[\"children\"]:\n",
    "\t\t\taux = aux[\"children\"][part]\n",
    "\t\telse:\n",
    "\t\t\taux[\"children\"][part] = {\"count\": 0, \"children\": dict(), \"words\": \"\", \"index\": -1}\n",
    "\t\t\taux = aux[\"children\"][part]\n",
    "\taux[\"count\"] = all_ingredients[line]\n",
    "\taux[\"words\"] = line\n",
    "\taux[\"index\"] = index\n",
    "\tindex+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Auxiliar functions\n",
    "def extract_nutrition_facts(recipe):\n",
    "\textracted=[]\n",
    "\tservings = recipe[\"yield\"]\n",
    "\tfor i in range(0, len(valid_nutrition_facts)):\n",
    "\t\tfact = valid_nutrition_facts[i]\n",
    "\t\tif fact in recipe[\"totalDaily\"]:\n",
    "\t\t\tin_recipe = recipe[\"totalDaily\"][fact]\n",
    "\t\t\textracted.append([i, in_recipe[\"quantity\"]/(100*servings)])\n",
    "\treturn extracted\n",
    "\n",
    "def extract_ingredient_facts(recipe):\n",
    "\textracted=[]\n",
    "\ttotal_weight=0\n",
    "\tfor ingredient_line in recipe[\"ingredients\"]:\n",
    "\t\tingredient = get_ingredient(ingredient_line[\"text\"], ingredient_map)\n",
    "\t\ttotal_weight+=ingredient_line[\"weight\"]\n",
    "\t\tif ingredient is None:\n",
    "\t\t\ttokenized = lemmatize_sentence(ingredient_line[\"text\"], True)\n",
    "\t\t\tfor i in range(0,len(tokenized)):\n",
    "\t\t\t\tingredient=\"\"\n",
    "\t\t\t\tanyNN=False\n",
    "\t\t\t\tfor j in range(i, min(len(tokenized), i+3)):\n",
    "\t\t\t\t\tif tokenized[j][1] == \"CD\" or tokenized[j][1] == \",\" or tokenized[j][1] == \".\" or (tokenized[j][2] in invalid_tokens):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tanyNN = anyNN or is_noun(tokenized[j][1])\n",
    "\t\t\t\t\tif i==j:\n",
    "\t\t\t\t\t\tingredient = tokenized[j][2].lower()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tingredient += \" \"+tokenized[j][2].lower()\n",
    "\t\t\t\t\tif anyNN:\n",
    "\t\t\t\t\t\tif ingredient not in unknown_ingredients:\n",
    "\t\t\t\t\t\t\tunknown_ingredients[ingredient]=1\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tunknown_ingredients[ingredient]=unknown_ingredients[ingredient]+1\n",
    "\t\telse:\n",
    "\t\t\t# Build the ingredient map\n",
    "\t\t\textracted.append([ingredient[\"index\"], ingredient_line[\"weight\"]])\n",
    "\tfor ext in extracted:\n",
    "\t\text[1] = ext[1]/total_weight\n",
    "\treturn extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/carlos/Documentos/idss/idss_pw3_food/food'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e970dc37764b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;31m# print(\"Reading file \"+filename)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mrecipes_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/carlos/Documentos/idss/idss_pw3_food/food'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Read files\n",
    "for filename in os.listdir(base_dir):\n",
    "\t# print(\"Reading file \"+filename)\n",
    "\twith open(base_dir+\"/\"+filename) as json_data:\n",
    "\t\trecipes_results = json.load(json_data)\n",
    "\t\tfor hit in recipes_results[\"hits\"]:\n",
    "\t\t\trecipe = hit[\"recipe\"]\n",
    "\t\t\tlabel = recipe[\"label\"]\n",
    "\t\t\tif label not in all_recipes:\n",
    "\t\t\t\tidss_recipe = {\"image\": recipe[\"image\"],\n",
    "\t\t\t\t\"url\": recipe[\"url\"],\n",
    "\t\t\t\t\"dietLabels\": recipe[\"dietLabels\"],\n",
    "\t\t\t\t\"healthLabels\": recipe[\"healthLabels\"],\n",
    "\t\t\t\t\"ingredients\": extract_ingredient_facts(recipe), \n",
    "\t\t\t\t\"name\": label, \n",
    "\t\t\t\t\"nutrition\": extract_nutrition_facts(recipe)}\n",
    "\t\t\t\tall_recipes[label] = idss_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save everything to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_ingredients = sorted(unknown_ingredients.items(), key=operator.itemgetter(1))\n",
    "with open('additional-ingredients.txt', 'w') as ingredients_file:\n",
    "\tfor ingredient in unknown_ingredients:\n",
    "\t\tingredients_file.write(str(ingredient[1])+\" \"+ingredient[0]+\"\\n\")\n",
    "with open('recipes.txt', 'w') as recipes_file:\n",
    "\trecipes_file.write(\"\\n\".join(all_recipes.keys()))\n",
    "with open('recipes.json', 'w') as outfile:\n",
    "    json.dump(all_recipes, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}